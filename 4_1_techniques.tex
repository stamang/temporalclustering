\section{Representations}
Most all temporal analysis methods integrate an abstraction techniques.  Some researchers argue that it is the single most important step.  That is, if the quality of the approximation is high, the results that are achieved will be similar to what would be achieved using the raw data.  If abstraction is poor, dynamics are not sufficiently captured and can lead to false findings.

Even the ideal abstraction technique can lead to erroneous findings, so we do not make this case.  However, it is obvious that the choice of high-level representation is a key decision in any temporal mining framework, and especially important when data is irregularly sampled and noisy.  In this case, abstraction can help to mitigate the impact of noise, and transform uneven length observation lengths into a uniform format that allows for further processing.

One way to describe an abstraction approach is in terms of how a time series is input for a learning task:

\begin{description}
  \item[Raw Data] In general, raw data it not feasibly processed by clustering methods.  For small, structured, or synthetically derived data, clustering on the raw data sequence is possible.
  \item[Feature-based] Many feature extraction methods are simple in nature and help to characterize the shape of the trajectory that is monitored.  They aim to measure a quality about the observation sequence instead of the underlying data generation process and use the similarity among features for clustering.  Also, features tend to be domain, task and data set specific.  For example, a features that works well for one data set may be irrelevant to another application.  Some well-known approaches to feature extraction that have continued to be enhanced since their introduction are spectral techniques.  These are borrowed from signal-processing and include Discrete Fourier Transform (DFT)~\cite{HarrisF}, Discrete Wavelet Transforms (DWT)~\cite{Shensa}, and Piecewise Linear Approximation (PLA)~\cite{Dunham}.
  \item[Model-based] Working under the assumption that each time series was generated by an underlying process that can be modeled, the similarity between different models serves as the basis for time series clustering.  There have been various methods proposed for model-based abstraction in the recent literature and applications of probabilistic Graphical models (PGMs) for temporal modeling is an increasingly active active area of research~\cite{Jebara_07,Smyth97,Fruhwirth11}.  These models represent observable or partially observable problem elements as nodes and uses directed edges to indicate the probabilistic dependencies, or relationships, among nodes.  These dependences can be represented with one time slice to indicated conditional distributions associated with the process, and in the case of dynamic Bayesian networks (DBNs) between time slices in the direction of time.  One aspect of DBNs, a specific instance PGMs, that makes them particularly attractive for modeling sequential phenomena is the expressive language that can be applied to describe a process.  Also, that problem semantics and algorithmic components are distinct.  For example, when an analysis for a new data set is required, a DBN structure will need to be defined to represent the new problem's semantics, but all of the algorithms parts lend themselves to immediate reuse.
\end{description}

\subsection{Subsequence and Whole Sequence Approaches}

In addition to abstraction types, we can also describe an abstraction method in terms of the information represented in the approximated values.   In the past, work with subsequences was pervasive.  However, a study by Keogh et al. showed the limitations of subsequence-based approaches resulted into a shift towards whole sequence analysis.  Although there has since been work that addresses the shortcomings noted in this key paper, for many domains and applications these findings still hold relevance.
\begin{description}
  \item [Whole sequence]
  \item [Subsequence]
\end{description}

\section{Moving Average Models}

The most widely used family of techniques for modeling time are autoregressive techniques, known collectively as autoregressive moving average or ARMA models.  They make beneficial assumption to model the dependencies between adjacent observations time series, but don't directly represent problem semantics rather features of the time series.  The name comes from the combination of autoregressive (AR) features with moving average (MA) models to from ARMA(p,q) models, and use a predetermined, fixed size temporal window that slides along the entire duration of the time series.

 \emph{Autoregressive} (AR) models assume a current value depends on previous periods $p$ and white noise $\epsilon_t$.  For a model of order $p$, with model parameters $\phi$ at time $t$ $AR(p)$ is defined as:
 $$y_t= \sum_{i=1}^{p}\phi_{i}y_{t-i}+\epsilon_t$$

 A \emph{moving average} models of order $q, MA(q)$ with parameters $\theta$ and white noise $\epsilon_t$ is defined as:
 $$y_t= \epsilon_t+\sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}$$

 ARMA models combine the two model types $AR(p)$ and $MA(q)$ and define $ARMA(p,q)$ by:

$$y_t= \epsilon_t + \sum_{i=1}^{p}\phi_{i}y_{t-i}+\sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}$$

Strengths of ARMA techniques for time series modeling includes their ease of interpretation, prediction quality, and their ability to extend to the multivariate settings.  However, stationarity, or the assumption that the mean, variance and autocorrelation is uniform for the duration of $T$, is a beneficial property of autoregressive models that is not well matched with real-world problems that are often more complex.  Additional requirements of ARMA models are typically that the developer must have extensive knowledge and experience with the process that generated the data, and the technique requires a substantial amount of data preprocessing and parameter tuning.

\input{4_3_features}
\input{4_4_graphical}




